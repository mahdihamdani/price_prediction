{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from  sklearn import linear_model, tree\n",
    "import sklearn.ensemble as se\n",
    "from sklearn import svm\n",
    "from category_encoders import *\n",
    "from bisect import bisect_left\n",
    "\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The app predicts the product price given a description and meta information about the product\n",
    "Let's use the given information in the collected data and how can we use it:\n",
    "\n",
    "- Restaurant information: this will be discarded, if the user knows the restaurant it will be a simple lookup,\n",
    "no need for predictio\n",
    "- Product information: this is the most important information we can extract, \n",
    "we will try using these fields for prediction: menu_category, product_name, product_description, \n",
    "dietary_characteristics, cuisine_characteristics, taste_characteristics, preparation_style_characteristics, \n",
    "dish_type_characteristics and ingredients\n",
    "- Place you want to eat/order: this also can be very useful. \n",
    "From a personal experience: eating in Berlin is more expensive than eating in Aachen :)\n",
    "Fields: postcode,latitude,longitude\n",
    "\n",
    "Let's classify the features now using their types --> different variable types\n",
    "- Fields with free text: menu_category, product_name, product_description\n",
    "- Fields with a list of text values: dietary_characteristics, cuisine_characteristics, taste_characteristics, \n",
    "cuisine_characteristics, taste_characteristics, preparation_style_characteristics, dish_type_characteristics\n",
    "ingredients\n",
    "- Fields with categories: postcode, city_id\n",
    "- Fields with numbers: latitude, longitude\n",
    "'''\n",
    "\n",
    "def conv_list(x):\n",
    "    '''\n",
    "        Helper to convert string list fields to actual lists\n",
    "        Args:\n",
    "            x: A string representing the list\n",
    "        Returns: list(str): A sorted list of strings converted to lower case\n",
    "    '''\n",
    "    try:\n",
    "        #transforming string to string list \n",
    "        return eval(x)\n",
    "        #sorting the list (TODO: remove if sure that it is already sorted)\n",
    "        #return sorted([s.lower() for s in l])\n",
    "    except SyntaxError as e:\n",
    "        return np.nan\n",
    "\n",
    "str_fields = ['product_name', 'menu_category', 'product_description']\n",
    "list_fields = ['dietary_characteristics', 'cuisine_characteristics', 'taste_characteristics', \\\n",
    "               'preparation_style_characteristics', 'dish_type_characteristics', 'ingredients']\n",
    "cat_fields = ['postcode', 'city_id']\n",
    "num_fields = ['latitude', 'longitude']\n",
    "pred_fields = ['price']\n",
    "\n",
    "feature_fields = str_fields+list_fields+cat_fields+num_fields\n",
    "\n",
    "converters = dict(zip(list_fields, [conv_list] * len(list_fields)))\n",
    "\n",
    "data = pd.read_csv('dataset_sub.csv', sep=';', usecols=feature_fields + pred_fields, \\\n",
    "                  converters=converters)\n",
    "\n",
    "'''\n",
    "Analysing missing data\n",
    "- Remove columns with moree than 65% missing data\n",
    "'''\n",
    "\n",
    "for feature_type in str_fields, list_fields, cat_fields, num_fields:\n",
    "    for col in feature_type:\n",
    "        percent_missing = float(data[col].isnull().sum()) / float(data.shape[0])\n",
    "        #print col, 'is missing', percent_missing, '% of the data'\n",
    "        if percent_missing > 0.65:\n",
    "            print 'Removing column', col\n",
    "            data.drop(col, axis=1)\n",
    "            feature_type.remove(col)\n",
    "            feature_fields.remove(col)\n",
    "print\n",
    "\n",
    "#removing all rows with nan in log or lattitude\n",
    "data.dropna(subset=num_fields, inplace=True)\n",
    "\n",
    "'''\n",
    "Splitting the dataset to train, dev and test sets\n",
    "We will try many data transformations --> better to keep test data for final tets to avoid \"overfitting\" on dev data\n",
    "'''\n",
    "\n",
    "x_train, x_all_test, y_train, y_all_test = train_test_split(data[feature_fields], data['price'], \\\n",
    "                                                            test_size=0.2, random_state=1234)\n",
    "x_dev, x_test, y_dev, y_test = train_test_split(x_all_test, y_all_test,test_size=0.5, random_state=1234)\n",
    "\n",
    "print 'Summary of used data:'\n",
    "print 'Train data size', x_train.shape[0], 'rows'\n",
    "print 'Dev data size', x_dev.shape[0], 'rows'\n",
    "print 'Test data size', x_test.shape[0], 'rows'\n",
    "print\n",
    "\n",
    "'''\n",
    "Using RandomForestRegressor with 3 trees for testing the data tranformations\n",
    "'''\n",
    "\n",
    "clf = se.RandomForestRegressor(n_estimators=3, n_jobs=-1)\n",
    "\n",
    "\n",
    "'''\n",
    "Set these to True to run the testing of the different features \n",
    "If set to True it will take a lot of time for hyperparameter search\n",
    "\n",
    "'''\n",
    "\n",
    "test_str = False\n",
    "test_list = False\n",
    "test_cat = False\n",
    "test_num = False\n",
    "test_regressors = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Let's start by transforming the string fields, these are free text fields so we will use a text analyzer\n",
    "Bag of n-grams can be used in this case: \n",
    "The context is important in this case, e.g. Pizza Salami vs Pizza al Tonno\n",
    "'''\n",
    "\n",
    "# {'column_name', 'param_value'}\n",
    "best_solutions = {}\n",
    "# {'column_name', 'best_mse'}\n",
    "best_results = dict(zip(feature_fields, [sys.float_info.max] * len(feature_fields)))\n",
    "best_results['lat_long'] = sys.float_info.max\n",
    "best_results['regressor'] = sys.float_info.max\n",
    "\n",
    "if test_str:\n",
    "    for n in range(1, 3):\n",
    "        #strip_accents: using unicode gave better results with German\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, n), min_df=1, strip_accents='unicode')\n",
    "        for col in str_fields:\n",
    "            x_train_feats = vectorizer.fit_transform(x_train[col].replace(np.nan, '', regex=True))\n",
    "            x_dev_feats = vectorizer.transform(x_dev[col].replace(np.nan, '', regex=True))\n",
    "\n",
    "            clf.fit(x_train_feats, y_train)\n",
    "            train_hyp = clf.predict(x_train_feats)\n",
    "            dev_hyp = clf.predict(x_dev_feats)\n",
    "            mse_dev = mean_squared_error(y_dev, dev_hyp)\n",
    "            if mse_dev < best_results[col]:\n",
    "                best_results[col] = mse_dev\n",
    "                best_solutions[col] = n\n",
    "            print 'Results using feature', col, 'and n-gram level', n\n",
    "            print 'Train MSE=', mean_squared_error(y_train, train_hyp)\n",
    "            print 'Dev MSE=', mse_dev\n",
    "            print 'Dev R2 score=',r2_score(y_dev, dev_hyp)\n",
    "            print\n",
    "\n",
    "    for col in str_fields:\n",
    "        print 'best n-gram for column', col, 'is', best_solutions[col]\n",
    "else:\n",
    "    best_solutions['product_name'] = 1\n",
    "    best_solutions['menu_category'] = 1\n",
    "    best_solutions['product_description'] = 2\n",
    "    \n",
    "'''\n",
    "1-grams were slightly better since the context is not large, will continue with that\n",
    "- product_name and product_description are more important here but they may contain redundant information\n",
    "- menu_category: seems to contain some useful information\n",
    "Let's test the combination and visualize the feature importance\n",
    "'''\n",
    "\n",
    "x_train_feats = None\n",
    "x_dev_feats = None\n",
    "x_test_feats = None\n",
    "\n",
    "for col in str_fields:\n",
    "    n_gram_level = best_solutions[col]\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, n_gram_level), min_df=1, strip_accents='unicode')\n",
    "    x_train_col = vectorizer.fit_transform(x_train[col].replace(np.nan, '', regex=True))\n",
    "    x_dev_col = vectorizer.transform(x_dev[col].replace(np.nan, '', regex=True))\n",
    "    x_test_col = vectorizer.transform(x_test[col].replace(np.nan, '', regex=True))\n",
    "    if x_train_feats is None:\n",
    "        x_train_feats = x_train_col\n",
    "        x_dev_feats = x_dev_col\n",
    "        x_test_feats = x_test_col\n",
    "    else:\n",
    "        x_train_feats = hstack([x_train_feats, x_train_col])\n",
    "        x_dev_feats = hstack([x_dev_feats, x_dev_col])\n",
    "        x_test_feats = hstack([x_test_feats, x_test_col])\n",
    "        \n",
    "clf.fit(x_train_feats, y_train)\n",
    "train_hyp = clf.predict(x_train_feats)\n",
    "dev_hyp = clf.predict(x_dev_feats)\n",
    "\n",
    "base_dev_mse = mean_squared_error(y_dev, dev_hyp)\n",
    "print 'Results using bag of words features'\n",
    "print 'Train MSE=', mean_squared_error(y_train, train_hyp)\n",
    "print 'Dev MSE=', base_dev_mse\n",
    "print 'Dev R2 score=',r2_score(y_dev, dev_hyp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Let's go now further with fields containing string lists\n",
    "We will try them one by one using categorical features\n",
    "'''\n",
    "\n",
    "def contains(a, v):\n",
    "    '''\n",
    "        Helper to search value x in array a using binary search\n",
    "        Args:\n",
    "            a: sorted array\n",
    "            v: value to search\n",
    "        Returns: Boolean: True if v exists in a\n",
    "    '''\n",
    "    i = bisect_left(a, v)\n",
    "    if i != len(a) and a[i] == v:\n",
    "        return i\n",
    "    return -1\n",
    "\n",
    "def indicatorFeature(in_list):\n",
    "    '''\n",
    "        Helper for one hot encoding for the lists\n",
    "        Args:\n",
    "            x: input list of strings\n",
    "        Returns: list(string): one hot encoding of list\n",
    "    '''\n",
    "    result = [0] * len(cols)\n",
    "    if not isinstance(in_list, list): return result\n",
    "    for value in in_list:\n",
    "        i = contains(cols, value)\n",
    "        if i >= 0:\n",
    "            result[i] = 1\n",
    "    return result\n",
    "\n",
    "transformers = [BackwardDifferenceEncoder, BinaryEncoder, \\\n",
    "                HashingEncoder, HelmertEncoder, OneHotEncoder, \\\n",
    "                OrdinalEncoder, PolynomialEncoder, SumEncoder, \\\n",
    "                BaseNEncoder, LeaveOneOutEncoder]\n",
    "\n",
    "if test_list:\n",
    "    for col in list_fields:\n",
    "        values = []\n",
    "        for v in x_train[col].values.tolist():\n",
    "            if v is np.nan: continue\n",
    "            values += [s for s in v]\n",
    "        cols = sorted(set(values))\n",
    "\n",
    "        x_train_feats_col = x_train[col].apply(indicatorFeature).apply(pd.Series)\n",
    "        x_dev_feats_col = x_dev[col].apply(indicatorFeature).apply(pd.Series)\n",
    "\n",
    "        for transformer in transformers:\n",
    "            tr = transformer()\n",
    "            tr.fit(x_train_feats_col, y_train)\n",
    "            x_train_feats_col = tr.transform(x_train_feats_col)\n",
    "            x_dev_feats_col = tr.transform(x_dev_feats_col)\n",
    "\n",
    "            if x_train_feats_col.shape[1] is not x_dev_feats_col.shape[1]:\n",
    "                print 'Ignoring', str(transformer), 'for column', col\n",
    "                continue\n",
    "\n",
    "            x_train_feats_test = hstack([x_train_feats, x_train_feats_col])\n",
    "            x_dev_feats_test = hstack([x_dev_feats, x_dev_feats_col])\n",
    "\n",
    "            clf.fit(x_train_feats_test, y_train)\n",
    "            train_hyp = clf.predict(x_train_feats_test)\n",
    "            dev_hyp = clf.predict(x_dev_feats_test)\n",
    "            \n",
    "            mse_dev = mean_squared_error(y_dev, dev_hyp)\n",
    "            if mse_dev < best_results[col]:\n",
    "                best_results[col] = mse_dev\n",
    "                best_solutions[col] = tr\n",
    "                \n",
    "            print 'Results using indicator features for column', col, 'with transformer', str(transformer)\n",
    "            print 'Train MSE=', mean_squared_error(y_train, train_hyp)\n",
    "            print 'Dev MSE=', mean_squared_error(y_dev, dev_hyp)\n",
    "            print 'Dev R2 score=',r2_score(y_dev, dev_hyp)\n",
    "            print\n",
    "            \n",
    "    for col in list_fields:\n",
    "        if col in best_solutions:\n",
    "            print 'best transformer for column', col, 'is', best_solutions[col]\n",
    "            if best_results[col] > base_dev_mse:\n",
    "                print 'Warning: worse results than baseline with column', col \n",
    "        else:\n",
    "            print 'Warning: transforming', col, 'failed with all transformers'\n",
    "            list_fields.remove(col)\n",
    "                \n",
    "else:\n",
    "    best_solutions['cuisine_characteristics'] = OrdinalEncoder()\n",
    "    best_solutions['taste_characteristics'] = BackwardDifferenceEncoder() \n",
    "    best_solutions['preparation_style_characteristics'] = LeaveOneOutEncoder()\n",
    "    best_solutions['dish_type_characteristics'] = PolynomialEncoder()\n",
    "    list_fields.remove('ingredients')\n",
    "    \n",
    "'''\n",
    "All features did not hurt performance or gave better results \n",
    "we will keep them and refince with feature selection later\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "We will work now with categorical features with different categorical feature representations\n",
    "Using the category_encoders library module as well\n",
    "'''\n",
    "\n",
    "if test_cat:\n",
    "    for col in cat_fields:\n",
    "        for transformer in transformers:\n",
    "            tr = transformer()\n",
    "            tr.fit(x_train[col].to_frame(), y_train)\n",
    "            x_train_feats_col = tr.transform(x_train[col].to_frame())\n",
    "            x_dev_feats_col = tr.transform(x_dev[col].to_frame())\n",
    "\n",
    "            if x_train_feats_col.shape[1] is not x_dev_feats_col.shape[1]:\n",
    "                print 'Ignoring', str(transformer), 'for column', col\n",
    "                continue\n",
    "\n",
    "            x_train_feats_col.fillna(0, inplace=True)\n",
    "            x_dev_feats_col.fillna(0, inplace=True)\n",
    "\n",
    "            x_train_feats_test = hstack([x_train_feats, x_train_feats_col])\n",
    "            x_dev_feats_test = hstack([x_dev_feats, x_dev_feats_col])\n",
    "\n",
    "            clf.fit(x_train_feats_test, y_train)\n",
    "            train_hyp = clf.predict(x_train_feats_test)\n",
    "            dev_hyp = clf.predict(x_dev_feats_test)\n",
    "\n",
    "            mse_dev = mean_squared_error(y_dev, dev_hyp)\n",
    "            if mse_dev < best_results[col]:\n",
    "                best_results[col] = mse_dev\n",
    "                best_solutions[col] = tr\n",
    "\n",
    "            print 'Results using feature', col, 'with transformer', str(transformer)\n",
    "            print 'Train MSE=', mean_squared_error(y_train, train_hyp)\n",
    "            print 'Dev MSE=', mse_dev\n",
    "            print 'Dev R2 score=',r2_score(y_dev, dev_hyp)\n",
    "            print\n",
    "            \n",
    "    for col in cat_fields:\n",
    "        print 'best transformer for column', col, 'is', best_solutions[col]\n",
    "else:\n",
    "    best_solutions['postcode'] = BinaryEncoder()\n",
    "    best_solutions['city_id'] = BinaryEncoder()\n",
    "\n",
    "'''\n",
    "Best results for the categorical features:\n",
    "postcode: BackwardDifferenceEncoder\n",
    "city_id: LeaveOneOutEncoder\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We will continue now with the logitude/lattitude features\n",
    "The idea is to have clusters places using the \n",
    "'''\n",
    "from sklearn.cluster import DBSCAN\n",
    "import scipy as sp\n",
    "from haversine import haversine\n",
    "\n",
    "x_train_places = np.radians(x_train[num_fields])\n",
    "x_dev_places = np.radians(x_dev[num_fields])\n",
    "x_test_places = np.radians(x_test[num_fields])\n",
    "\n",
    "kms_per_radian = 6371.0088\n",
    "epsilon = 1.0 / kms_per_radian\n",
    "dbscan_model = DBSCAN(eps=epsilon, min_samples=1, algorithm='ball_tree', metric='haversine')\n",
    "\n",
    "x_train_clusters = dbscan_model.fit_predict(x_train_places)\n",
    "x_train_clusters = np.resize(x_train_clusters, (x_train_clusters.shape[0], 1))\n",
    "\n",
    "def dbscan_predict(points, metric=haversine):\n",
    "    '''\n",
    "        Helper to find closest cluster\n",
    "        Args:\n",
    "            dbscan_model: trained sklearn DBSCAN model\n",
    "            X_new np.array: lat/long in radians\n",
    "            metric: used metric for comparing points\n",
    "        Returns: np.array: cluster values\n",
    "    '''\n",
    "    # Result is noise by default\n",
    "    cluster = np.ones(shape=len(points), dtype=int)*-1\n",
    "\n",
    "    # Iterate all input samples for a label\n",
    "    for j, point in enumerate(points):\n",
    "        # Find a core sample closer than EPS\n",
    "        for i, x_core in enumerate(dbscan_model.components_):\n",
    "            if metric(point, x_core) < dbscan_model.eps:\n",
    "                # Assign label\n",
    "                cluster[j] = dbscan_model.labels_[dbscan_model.core_sample_indices_[i]]\n",
    "                break\n",
    "    return cluster\n",
    "\n",
    "x_dev_clusters = dbscan_predict(x_dev_places.as_matrix())\n",
    "x_dev_clusters = np.resize(x_dev_clusters, (x_dev_clusters.shape[0], 1))\n",
    "\n",
    "x_test_clusters = dbscan_predict(x_test_places.as_matrix())\n",
    "x_test_clusters = np.resize(x_test_clusters, (x_test_clusters.shape[0], 1))\n",
    "\n",
    "if test_num:\n",
    "    for transformer in transformers:\n",
    "            tr = transformer()\n",
    "            tr.fit(x_train_clusters, y_train)\n",
    "            x_train_feats_col = tr.transform(x_train_clusters)\n",
    "            x_dev_feats_col = tr.transform(x_dev_clusters)\n",
    "             \n",
    "            if x_train_feats_col.shape[1] is not x_dev_feats_col.shape[1]:\n",
    "                print 'Ignoring', str(transformer), 'for latitude/longitude'\n",
    "                continue\n",
    "                \n",
    "            x_train_feats_test = hstack([x_train_feats, x_train_feats_col])\n",
    "            x_dev_feats_test = hstack([x_dev_feats, x_dev_feats_col])\n",
    "\n",
    "            clf.fit(x_train_feats_test, y_train)\n",
    "            train_hyp = clf.predict(x_train_feats_test)\n",
    "            dev_hyp = clf.predict(x_dev_feats_test)\n",
    "            \n",
    "            mse_dev = mean_squared_error(y_dev, dev_hyp)\n",
    "            if mse_dev < best_results['lat_long']:\n",
    "                best_results['lat_long'] = mse_dev\n",
    "                best_solutions['lat_long'] = tr\n",
    "\n",
    "            print 'Results using logitude/lattitude with transformer', transformer\n",
    "            print 'Train MSE=', mean_squared_error(y_train, train_hyp)\n",
    "            print 'Dev MSE=', mse_dev\n",
    "            print 'Dev R2 score=',r2_score(y_dev, dev_hyp)\n",
    "            print\n",
    "    print 'best transformer for column', col, 'is', str(best_solutions['lat_long'])\n",
    "else:\n",
    "    best_solutions['lat_long'] = HashingEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Testing different regressors using the selected columns / parameters\n",
    "\n",
    "'''\n",
    "for col in list_fields:\n",
    "    values = []\n",
    "    for v in x_train[col].values.tolist():\n",
    "        if v is np.nan: continue\n",
    "        values += [s for s in v]\n",
    "    cols = sorted(set(values))\n",
    "\n",
    "    x_train_feats_col = x_train[col].apply(indicatorFeature).apply(pd.Series)\n",
    "    x_dev_feats_col = x_dev[col].apply(indicatorFeature).apply(pd.Series)\n",
    "    x_test_feats_col = x_test[col].apply(indicatorFeature).apply(pd.Series)\n",
    "    transformer = best_solutions[col]\n",
    "    transformer.fit(x_train_feats_col, y_train)\n",
    "\n",
    "    x_train_feats_col = transformer.transform(x_train_feats_col)\n",
    "    x_dev_feats_col = transformer.transform(x_dev_feats_col)\n",
    "    x_test_feats_col = transformer.transform(x_test_feats_col)\n",
    "\n",
    "    if x_train_feats_col.shape[1] is not x_dev_feats_col.shape[1] or \\\n",
    "            x_dev_feats_col.shape[1] is not x_test_feats_col.shape[1]:\n",
    "        print 'Warning: Ignoring column', col\n",
    "        continue\n",
    "\n",
    "    x_train_feats = hstack([x_train_feats, x_train_feats_col])\n",
    "    x_dev_feats = hstack([x_dev_feats, x_dev_feats_col])\n",
    "    x_test_feats = hstack([x_test_feats, x_test_feats_col])\n",
    "\n",
    "for col in cat_fields:\n",
    "    transformer = best_solutions[col]\n",
    "    transformer.fit(x_train[col].to_frame(), y_train)\n",
    "    x_train_feats_col = transformer.transform(x_train[col].to_frame())\n",
    "    x_dev_feats_col = transformer.transform(x_dev[col].to_frame())\n",
    "    x_test_feats_col = transformer.transform(x_test[col].to_frame())\n",
    "\n",
    "    if x_train_feats_col.shape[1] is not x_dev_feats_col.shape[1]:\n",
    "        print 'Warning: Ignoring column', col\n",
    "        continue\n",
    "\n",
    "    x_train_feats_col.fillna(0, inplace=True)\n",
    "    x_dev_feats_col.fillna(0, inplace=True)\n",
    "    x_test_feats_col.fillna(0, inplace=True)\n",
    "\n",
    "    x_train_feats = hstack([x_train_feats, x_train_feats_col])\n",
    "    x_dev_feats = hstack([x_dev_feats, x_dev_feats_col])\n",
    "    x_test_feats = hstack([x_test_feats, x_test_feats_col])\n",
    "\n",
    "transformer = best_solutions['lat_long']\n",
    "transformer.fit(x_train_clusters, y_train)\n",
    "x_train_feats_col = transformer.transform(x_train_clusters)\n",
    "x_dev_feats_col = transformer.transform(x_dev_clusters)\n",
    "x_dev_test_col = transformer.transform(x_test_clusters)\n",
    "\n",
    "if x_train_feats_col.shape[1] is not x_dev_feats_col.shape[1]:\n",
    "    print 'Ignoring', str(transformer), 'for latitude/longitude'\n",
    "else:\n",
    "    x_train_feats = hstack([x_train_feats, x_train_feats_col])\n",
    "    x_dev_feats = hstack([x_dev_feats, x_dev_feats_col])\n",
    "    x_test_feats = hstack([x_test_feats, x_dev_test_col])\n",
    "\n",
    "'''\n",
    "Trying different regressors, RandomForestRegressor performs better on this data\n",
    "svm.SVR: Support Vector Classification extended to solve regression problems\n",
    "linear_model.SGDRegressor: Linear model fitted by minimizing a regularized empirical loss with SGD\n",
    "se.RandomForestRegressor: A random forest is a meta estimator that fits a number of classifying decision \n",
    "trees on various sub-samples\n",
    "'''\n",
    "\n",
    "if test_regressor:\n",
    "    regressors = [\n",
    "        svm.SVR(),\n",
    "        linear_model.SGDRegressor(),\n",
    "        se.RandomForestRegressor(n_jobs=-1)]\n",
    "\n",
    "    for regressor in regressors:\n",
    "        regressor.fit(x_train_feats, y_train)\n",
    "        train_hyp = regressor.predict(x_train_feats)\n",
    "        dev_hyp = regressor.predict(x_dev_feats)\n",
    "\n",
    "        mse_dev = mean_squared_error(y_dev, dev_hyp)\n",
    "        if mse_dev < best_results['regressor']:\n",
    "            best_results['regressor'] = mse_dev\n",
    "            best_solutions['regressor'] = regressor\n",
    "\n",
    "        print 'Results with regressor', type(regressor).__name__\n",
    "        print 'Train MSE=', mean_squared_error(y_train, train_hyp)\n",
    "        print 'Dev MSE=', mse_dev\n",
    "        print 'Dev R2 score=',r2_score(y_dev, dev_hyp)\n",
    "        print\n",
    "    print 'best regressor is', type(best_solutions['regressor']).__name__\n",
    "else:\n",
    "    best_solutions['regressor'] = se.RandomForestRegressor(n_jobs=-1)\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "save_obj(best_solutions, 'models/best_config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print x_train_feats.shape, x_test_feats.shape, x_dev_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing and plotting\n",
    "regressor = best_solutions['regressor']\n",
    "regressor.fit(x_train_feats, y_train)\n",
    "test_hyp = regressor.predict(x_test_feats)\n",
    "print 'Results with regressor', type(regressor).__name__\n",
    "print 'Test MSE=', mean_squared_error(y_test, test_hyp)\n",
    "print 'Test R2 score=',r2_score(y_dev, dev_hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, _ = plt.subplots(1, 1, sharey=True)\n",
    "p = pd.DataFrame({feature: x_test[feature], 'Hyp':test_hyp })\n",
    "p1 = pd.DataFrame({feature: x_test[feature], 'Ref':y_test })\n",
    "\n",
    "#p = p.set_index(feature, drop=False).groupby(feature)\n",
    "plt.ylabel('Price')\n",
    "plt.plot(p.index.values, p['Hyp'], '.r', label=\"Hypothesized prices\")\n",
    "plt.plot(p1.index.values, p1['Ref'], '.b', label=\"Reference prices\")\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3, ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
